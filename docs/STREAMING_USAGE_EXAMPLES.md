# æµå¼è¿”å›ä½¿ç”¨ç¤ºä¾‹

## æ¦‚è¿°

RimAI Framework ç°åœ¨æ”¯æŒæµå¼å’Œéæµå¼ä¸¤ç§ LLM è¯·æ±‚æ–¹å¼ã€‚æµå¼è¿”å›å…è®¸å®æ—¶æ¥æ”¶ AI å“åº”çš„æ¯ä¸ª token ç‰‡æ®µï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

**é‡è¦ï¼š** åœ¨æ¸¸æˆè®¾ç½®ä¸­æœ‰ä¸€ä¸ª"å¯ç”¨æµå¼ä¼ è¾“"é€‰é¡¹ï¼Œå½“å¯ç”¨æ—¶ï¼Œå³ä½¿è°ƒç”¨ `GetChatCompletionAsync` ä¹Ÿä¼šåœ¨å†…éƒ¨ä½¿ç”¨æµå¼ä¼ è¾“æ¥æé«˜å“åº”é€Ÿåº¦ï¼

## è®¾ç½®é€‰é¡¹

### æ¸¸æˆå†…è®¾ç½®ç•Œé¢

åœ¨ RimWorld çš„æ¨¡ç»„è®¾ç½®ä¸­ï¼Œæ‚¨ä¼šçœ‹åˆ°ï¼š

- â˜‘ï¸ **å¯ç”¨æµå¼ä¼ è¾“** - å½“å¯ç”¨æ—¶ï¼š
  - `GetChatCompletionAsync` åœ¨å†…éƒ¨ä½¿ç”¨æµå¼ä¼ è¾“ï¼ˆä½†ä»è¿”å›å®Œæ•´å“åº”ï¼‰
  - `GetChatCompletionStreamAsync` æ­£å¸¸å·¥ä½œï¼Œæä¾›å®æ—¶å›è°ƒ
  - å¯èƒ½æé«˜å“åº”é€Ÿåº¦å’Œé™ä½è¶…æ—¶é£é™©

- â˜ **å¯ç”¨æµå¼ä¼ è¾“** - å½“ç¦ç”¨æ—¶ï¼š
  - `GetChatCompletionAsync` ä½¿ç”¨ä¼ ç»Ÿçš„è¯·æ±‚-å“åº”æ¨¡å¼
  - `GetChatCompletionStreamAsync` ä»ç„¶å¯ç”¨ï¼ˆç‹¬ç«‹å·¥ä½œï¼‰

## ä½¿ç”¨æ–¹æ³•

### 0. æ£€æŸ¥æµå¼çŠ¶æ€ï¼ˆä¸‹æ¸¸ Mod é‡è¦ï¼ï¼‰

ä¸‹æ¸¸ Mod å¯ä»¥æ£€æŸ¥å½“å‰çš„æµå¼è®¾ç½®æ¥è°ƒæ•´ UI è¡Œä¸ºï¼š

```csharp
// æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å¯ç”¨äº†æµå¼ä¼ è¾“
bool isStreamingEnabled = LLMManager.Instance.IsStreamingEnabled;

if (isStreamingEnabled)
{
    // ç”¨æˆ·å¯ç”¨äº†æµå¼ï¼ŒGetChatCompletionAsync å†…éƒ¨ä½¿ç”¨æµå¼ä¼ è¾“
    // UI å¯ä»¥æ˜¾ç¤º"å¿«é€Ÿå“åº”æ¨¡å¼"æˆ–è€…ç±»ä¼¼æç¤º
    ShowQuickResponseIndicator();
}
else
{
    // ç”¨æˆ·ç¦ç”¨äº†æµå¼ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼
    // UI å¯ä»¥æ˜¾ç¤º"æ ‡å‡†æ¨¡å¼"æç¤º
    ShowStandardModeIndicator();
}

// ä¹Ÿå¯ä»¥è·å–å®Œæ•´çš„è®¾ç½®ä¿¡æ¯
var settings = LLMManager.Instance.CurrentSettings;
Log.Message($"å½“å‰ä½¿ç”¨æ¨¡å‹: {settings.modelName}");
Log.Message($"æµå¼æ¨¡å¼: {settings.enableStreaming}");
```

### å®é™…åº”ç”¨ç¤ºä¾‹ï¼š

```csharp
public class SmartAIDialog : Dialog
{
    private void ShowResponseModeInfo()
    {
        if (LLMManager.Instance.IsStreamingEnabled)
        {
            Widgets.Label(infoRect, "ğŸš€ å¿«é€Ÿå“åº”æ¨¡å¼å·²å¯ç”¨");
        }
        else
        {
            Widgets.Label(infoRect, "ğŸ“ æ ‡å‡†å“åº”æ¨¡å¼");
        }
    }
    
    private async void SendMessage(string message)
    {
        if (LLMManager.Instance.IsStreamingEnabled)
        {
            // ç”¨æˆ·å¯ç”¨äº†æµå¼ï¼Œå¯ä»¥æ˜¾ç¤º"æ­£åœ¨å¿«é€Ÿè·å–å“åº”..."
            statusText = "æ­£åœ¨å¿«é€Ÿè·å–å“åº”...";
        }
        else
        {
            // ä¼ ç»Ÿæ¨¡å¼ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            statusText = "æ­£åœ¨è·å–å“åº”ï¼Œè¯·ç¨å€™...";
        }
        
        var response = await LLMManager.Instance.GetChatCompletionAsync(message);
        // å¤„ç†å“åº”...
    }
}
```

### 1. éæµå¼è¯·æ±‚ï¼ˆç°æœ‰æ–¹å¼ï¼Œä¿æŒä¸å˜ï¼‰

```csharp
// ä¼ ç»Ÿçš„ä¸€æ¬¡æ€§è¿”å›å®Œæ•´å“åº”
var response = await LLMManager.Instance.GetChatCompletionAsync(
    "Tell me about RimWorld", 
    cancellationToken
);

if (response != null)
{
    Log.Message($"Complete response: {response}");
}
else
{
    Log.Warning("Request failed or returned null");
}
```

### 2. æµå¼è¯·æ±‚ï¼ˆæ–°åŠŸèƒ½ï¼‰

```csharp
// å®æ—¶æ¥æ”¶å“åº”ç‰‡æ®µ
StringBuilder fullResponse = new StringBuilder();

await LLMManager.Instance.GetChatCompletionStreamAsync(
    "Tell me a story about RimWorld colonists",
    chunk =>
    {
        // æ¯æ”¶åˆ°ä¸€ä¸ª token ç‰‡æ®µå°±ä¼šè°ƒç”¨è¿™ä¸ªå›è°ƒ
        fullResponse.Append(chunk);
        
        // å¯ä»¥å®æ—¶æ›´æ–° UI æ˜¾ç¤ºéƒ¨åˆ†å“åº”
        UpdateUIWithPartialResponse(fullResponse.ToString());
        
        // æˆ–è€…é€å­—ç¬¦æ˜¾ç¤ºæ•ˆæœ
        Log.Message($"Received chunk: '{chunk}'");
    },
    cancellationToken
);

Log.Message($"Streaming completed. Full response: {fullResponse}");
```

### 3. å®é™…æ¸¸æˆåœºæ™¯ç¤ºä¾‹

#### åœºæ™¯Aï¼šAI åŠ©æ‰‹å¯¹è¯çª—å£
```csharp
public class AIAssistantDialog : Dialog
{
    private StringBuilder currentResponse = new StringBuilder();
    private string displayText = "";
    
    private async void SendMessage(string userMessage)
    {
        currentResponse.Clear();
        
        await LLMManager.Instance.GetChatCompletionStreamAsync(
            userMessage,
            chunk =>
            {
                currentResponse.Append(chunk);
                displayText = currentResponse.ToString();
                // è§¦å‘ UI é‡ç»˜
                SetDirty();
            }
        );
    }
    
    public override void DoWindowContents(Rect inRect)
    {
        // æ˜¾ç¤ºå®æ—¶æ›´æ–°çš„å“åº”æ–‡æœ¬
        Widgets.Label(responseRect, displayText);
    }
}
```

#### åœºæ™¯Bï¼šç»ˆç«¯å¼å‘½ä»¤å“åº”
```csharp
public class AITerminal
{
    private List<string> terminalLines = new List<string>();
    private StringBuilder currentLine = new StringBuilder();
    
    public async Task ProcessCommand(string command)
    {
        currentLine.Clear();
        terminalLines.Add($"> {command}");
        
        await LLMManager.Instance.GetChatCompletionStreamAsync(
            command,
            chunk =>
            {
                currentLine.Append(chunk);
                
                // æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ
                if (currentLine.Length > 80) // æ¯è¡Œæœ€å¤š80å­—ç¬¦
                {
                    terminalLines.Add(currentLine.ToString());
                    currentLine.Clear();
                }
                
                // æ›´æ–°ç»ˆç«¯æ˜¾ç¤º
                RefreshTerminalDisplay();
            }
        );
        
        // æ·»åŠ æœ€åä¸€è¡Œ
        if (currentLine.Length > 0)
        {
            terminalLines.Add(currentLine.ToString());
        }
    }
}
```

#### åœºæ™¯Dï¼šæ™ºèƒ½ UI é€‚é…ï¼ˆæ¨èæ¨¡å¼ï¼‰
```csharp
public class AdaptiveAIInterface
{
    private string statusMessage = "";
    private bool showProgressBar = false;
    
    public async Task ProcessUserInput(string input)
    {
        // æ ¹æ®æµå¼è®¾ç½®è°ƒæ•´ UI è¡Œä¸º
        bool isStreaming = LLMManager.Instance.IsStreamingEnabled;
        
        if (isStreaming)
        {
            // æµå¼æ¨¡å¼ï¼šç”¨æˆ·æœŸæœ›å¿«é€Ÿå“åº”ï¼Œæ˜¾ç¤ºç®€æ´çš„çŠ¶æ€
            statusMessage = "ğŸš€ AI æ­£åœ¨å¿«é€Ÿæ€è€ƒ...";
            showProgressBar = false; // æµå¼æ¨¡å¼ä¸éœ€è¦è¿›åº¦æ¡
            
            // å¯ä»¥é€‰æ‹©ä½¿ç”¨çœŸæ­£çš„æµå¼APIæ¥æä¾›å®æ—¶åé¦ˆ
            var response = new StringBuilder();
            await LLMManager.Instance.GetChatCompletionStreamAsync(
                input,
                chunk => 
                {
                    response.Append(chunk);
                    statusMessage = $"âœï¸ AI: {response}";
                    RefreshUI();
                }
            );
        }
        else
        {
            // éæµå¼æ¨¡å¼ï¼šç”¨æˆ·çŸ¥é“éœ€è¦ç­‰å¾…ï¼Œæ˜¾ç¤ºè¯¦ç»†è¿›åº¦
            statusMessage = "ğŸ¤” AI æ­£åœ¨æ·±åº¦æ€è€ƒï¼Œè¯·ç¨å€™...";
            showProgressBar = true;
            
            // æ˜¾ç¤ºè¿›åº¦æ¡åŠ¨ç”»
            StartProgressAnimation();
            
            var response = await LLMManager.Instance.GetChatCompletionAsync(input);
            
            StopProgressAnimation();
            statusMessage = "âœ… å“åº”å®Œæˆ";
            
            if (response != null)
            {
                DisplayFullResponse(response);
            }
        }
    }
    
    private void RefreshUI()
    {
        // è§¦å‘ç•Œé¢é‡ç»˜
        Find.WindowStack.WindowOfType<AIDialog>()?.SetDirty();
    }
}
```

#### åœºæ™¯Eï¼šæ€§èƒ½ä¼˜åŒ–çš„èŠå¤©æœºå™¨äºº
```csharp
public class PerformanceOptimizedChatBot
{
    public async Task<string> GetAIResponse(string userMessage)
    {
        // æ£€æŸ¥ç”¨æˆ·è®¾ç½®ï¼Œä¼˜åŒ–ä¸åŒæ¨¡å¼ä¸‹çš„ä½“éªŒ
        if (LLMManager.Instance.IsStreamingEnabled)
        {
            // æµå¼æ¨¡å¼ï¼šåˆ©ç”¨å†…éƒ¨æµå¼ä¼ è¾“çš„ä¼˜åŠ¿
            Log.Message("Using streaming mode for better responsiveness");
            
            // ç›´æ¥ä½¿ç”¨ GetChatCompletionAsyncï¼Œå†…éƒ¨ä¼šä½¿ç”¨æµå¼ä¼ è¾“
            return await LLMManager.Instance.GetChatCompletionAsync(userMessage);
        }
        else
        {
            // éæµå¼æ¨¡å¼ï¼šå¯èƒ½éœ€è¦æ›´é•¿çš„è¶…æ—¶æ—¶é—´
            Log.Message("Using traditional mode, allowing longer timeout");
            
            using var cts = new CancellationTokenSource(TimeSpan.FromMinutes(2)); // æ›´é•¿çš„è¶…æ—¶
            return await LLMManager.Instance.GetChatCompletionAsync(userMessage, cts.Token);
        }
    }
}
```
```csharp
public class EventNarrator
{
    public async Task NarrateEvent(string eventContext)
    {
        var narrative = new StringBuilder();
        
        await LLMManager.Instance.GetChatCompletionStreamAsync(
            $"Narrate this RimWorld event: {eventContext}",
            chunk =>
            {
                narrative.Append(chunk);
                
                // æ¯æ”¶åˆ°å‡ ä¸ªè¯å°±æ˜¾ç¤ºä¸€æ¬¡æ¶ˆæ¯
                if (narrative.ToString().Split(' ').Length % 10 == 0)
                {
                    Messages.Message(
                        narrative.ToString(),
                        MessageTypeDefOf.NeutralEvent
                    );
                }
            }
        );
        
        // æ˜¾ç¤ºå®Œæ•´çš„äº‹ä»¶æè¿°
        Find.LetterStack.ReceiveLetter(
            "AI Narrator",
            narrative.ToString(),
            LetterDefOf.NeutralEvent
        );
    }
}
```

## æŠ€æœ¯ç»†èŠ‚

### æµå¼çŠ¶æ€æ£€æŸ¥
ä¸‹æ¸¸ Mod å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼äº†è§£å½“å‰çš„æµå¼é…ç½®ï¼š

```csharp
// æ£€æŸ¥æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“
bool streaming = LLMManager.Instance.IsStreamingEnabled;

// è·å–å®Œæ•´çš„é…ç½®ä¿¡æ¯
var settings = LLMManager.Instance.CurrentSettings;
string model = settings.modelName;
string endpoint = settings.apiEndpoint;
bool embeddings = settings.enableEmbeddings;
```

**é‡è¦ï¼š** å½“ `IsStreamingEnabled` ä¸º `true` æ—¶ï¼Œå³ä½¿è°ƒç”¨ `GetChatCompletionAsync` ä¹Ÿä¼šåœ¨å†…éƒ¨ä½¿ç”¨æµå¼ä¼ è¾“æ¥æé«˜æ€§èƒ½ã€‚ä¸‹æ¸¸ Mod å¯ä»¥æ®æ­¤è°ƒæ•´ UI æç¤ºå’Œç”¨æˆ·æœŸæœ›ã€‚

### çº¿ç¨‹å®‰å…¨
- æµå¼å›è°ƒè‡ªåŠ¨åœ¨ä¸»çº¿ç¨‹æ‰§è¡Œï¼Œç¡®ä¿ UI æ›´æ–°å®‰å…¨
- å¦‚æœä¸»çº¿ç¨‹è°ƒåº¦å¤±è´¥ï¼Œä¼šå›é€€åˆ°ç›´æ¥è°ƒç”¨ï¼ˆå¸¦å¼‚å¸¸ä¿æŠ¤ï¼‰

### é”™è¯¯å¤„ç†
- ç½‘ç»œé”™è¯¯ã€JSON è§£æé”™è¯¯ç­‰éƒ½è¢«å†…éƒ¨æ•è·
- æµå¼å¤„ç†ä¸­çš„å¼‚å¸¸ä¸ä¼šå½±å“æ¸¸æˆç¨³å®šæ€§
- å›è°ƒå‡½æ•°ä¸­çš„å¼‚å¸¸è¢«å•ç‹¬æ•è·å’Œè®°å½•

### æ€§èƒ½è€ƒè™‘
- æµå¼è¯·æ±‚ä¸éæµå¼è¯·æ±‚å…±äº«ç›¸åŒçš„å¹¶å‘æ§åˆ¶ï¼ˆæœ€å¤š3ä¸ªåŒæ—¶è¯·æ±‚ï¼‰
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–ï¼Œé¿å…å¤§å“åº”çš„ä¸€æ¬¡æ€§åŠ è½½
- æ”¯æŒå–æ¶ˆæ“ä½œï¼Œå¯ä»¥åŠæ—¶åœæ­¢é•¿æ—¶é—´çš„æµå¼å“åº”

### å…¼å®¹æ€§
- å®Œå…¨å‘åå…¼å®¹ï¼Œç°æœ‰çš„éæµå¼ä»£ç æ— éœ€ä¿®æ”¹
- æµå¼åŠŸèƒ½æ˜¯å¯é€‰çš„ï¼Œä¸ä½¿ç”¨æ—¶ä¸ä¼šæœ‰ä»»ä½•å½±å“
- æ”¯æŒæ‰€æœ‰ç¬¦åˆ OpenAI API æ ‡å‡†çš„ LLM æœåŠ¡

## æœ€ä½³å®è·µ

1. **æ£€æŸ¥æµå¼çŠ¶æ€**ï¼šåœ¨ UI ä¸­æ ¹æ® `IsStreamingEnabled` è°ƒæ•´ç”¨æˆ·æç¤ºå’ŒæœŸæœ›
2. **é€‚å½“çš„å›è°ƒé¢‘ç‡**ï¼šé¿å…åœ¨å›è°ƒä¸­æ‰§è¡Œè€—æ—¶æ“ä½œ
3. **å†…å­˜ç®¡ç†**ï¼šå¯¹äºé•¿å“åº”ï¼Œè€ƒè™‘é™åˆ¶æ˜¾ç¤ºæ–‡æœ¬é•¿åº¦
4. **ç”¨æˆ·ä½“éªŒ**ï¼šæä¾›åœæ­¢æŒ‰é’®ï¼Œå…è®¸ç”¨æˆ·å–æ¶ˆé•¿æ—¶é—´çš„æµå¼è¯·æ±‚
5. **é”™è¯¯å¤„ç†**ï¼šå§‹ç»ˆæ£€æŸ¥æµå¼è¯·æ±‚æ˜¯å¦æˆåŠŸå®Œæˆ
6. **UI æ›´æ–°**ï¼šåˆç†æ§åˆ¶ UI æ›´æ–°é¢‘ç‡ï¼Œé¿å…è¿‡åº¦é‡ç»˜
7. **è®¾ç½®æ„ŸçŸ¥**ï¼šè®©ç”¨æˆ·çŸ¥é“å½“å‰çš„å“åº”æ¨¡å¼ï¼Œè®¾ç½®åˆç†çš„ç­‰å¾…æœŸæœ›

### æ¨èçš„ä¸‹æ¸¸ Mod å®ç°æ¨¡å¼

```csharp
public class BestPracticeAIIntegration
{
    public async Task ProcessAIRequest(string prompt)
    {
        // 1. æ£€æŸ¥å½“å‰è®¾ç½®
        bool isStreaming = LLMManager.Instance.IsStreamingEnabled;
        
        // 2. æ ¹æ®è®¾ç½®è°ƒæ•´ UI
        UpdateUIForCurrentMode(isStreaming);
        
        // 3. é€‰æ‹©åˆé€‚çš„ API
        if (needRealTimeUpdates && isStreaming)
        {
            // éœ€è¦å®æ—¶æ›´æ–°ä¸”å¯ç”¨äº†æµå¼ - ä½¿ç”¨æµå¼ API
            await LLMManager.Instance.GetChatCompletionStreamAsync(prompt, OnChunkReceived);
        }
        else
        {
            // å…¶ä»–æƒ…å†µ - ä½¿ç”¨æ ‡å‡† APIï¼ˆå¯èƒ½å†…éƒ¨ä½¿ç”¨æµå¼ï¼‰
            var response = await LLMManager.Instance.GetChatCompletionAsync(prompt);
            OnResponseComplete(response);
        }
    }
    
    private void UpdateUIForCurrentMode(bool isStreaming)
    {
        if (isStreaming)
        {
            statusLabel.text = "âš¡ å¿«é€Ÿå“åº”æ¨¡å¼";
            timeoutWarning.SetActive(false);
        }
        else
        {
            statusLabel.text = "ğŸ“ æ ‡å‡†æ¨¡å¼";
            timeoutWarning.SetActive(true);
        }
    }
}
```

## æ³¨æ„äº‹é¡¹

- æµå¼è¯·æ±‚éœ€è¦ LLM æœåŠ¡æ”¯æŒ Server-Sent Events (SSE)
- å›è°ƒå‡½æ•°åº”è¯¥å°½å¿«æ‰§è¡Œå®Œæ¯•ï¼Œé¿å…é˜»å¡æµå¤„ç†
- åœ¨å›è°ƒä¸­ä¿®æ”¹ UI æ—¶è¦è€ƒè™‘çº¿ç¨‹å®‰å…¨
- æµå¼è¯·æ±‚çš„å–æ¶ˆå¯èƒ½ä¸ä¼šç«‹å³ç”Ÿæ•ˆï¼Œå–å†³äºç½‘ç»œçŠ¶å†µ

è¿™ä¸ªåŠŸèƒ½ä¸º RimWorld AI æ¨¡ç»„å¼€å‘æä¾›äº†æ›´ä¸°å¯Œçš„äº¤äº’å¯èƒ½æ€§ï¼
